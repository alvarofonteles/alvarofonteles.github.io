// ===== DADOS DOS PROJETOS DATA ENGINEERING =====
const PROJECTS_DE_DATA = {
    // Roadmap Completo
    roadmap: [
        {
            id: 0,
            title: "üèóÔ∏è Metodologia de Estudos",
            description: "Estrutura completa organizada em 9 fases para dominar Data Engineering moderno. Mostra minha abordagem sistem√°tica de aprendizado.",
            technologies: ["Organiza√ß√£o", "Metodologia", "Roadmap", "Planejamento", "Data Engineering"],
            links: [
                {
                    name: "üìÅ Estrutura do Projeto",
                    url: "https://1drv.ms/f/c/0cc82fec9c1ab050/Ej_zAkJcDsxPpKpQkODTdqYBUnji_yfcbF2PlTOsoQmfOA"
                }
            ],
            featured: true,
            status: "completed",  // Porque a organiza√ß√£o j√° est√° feita!
            images: [ "/projects/assets/images/badges/methodology-badge.svg" ]     
        },
        {
            id: 1,
            title: "üêç FASE 1 - Python Fundamentos",
            description: "Fundamentos Python completos: desde estruturas de dados e fun√ß√µes at√© programa√ß√£o orientada a objetos com dataclasses, metaclasses, m√≥dulos e constru√ß√£o de APIs REST com autentica√ß√£o JWT/OAuth2 e banco de dados.",
            technologies: ["Python", "Data Structures", "Functions", "Dictionary", "OOP", "Modules & Packages", "Dataclasses", "Metaclass", "REST APIs", "SQLAlchemy", "Pydantic", "JSON", "JWT", "OAuth2"],
            links: [
                { name: "üîó python_teo", url: "https://github.com/alvarofonteles/python_teo" },
                { name: "üîó py_functions_duno", url: "https://github.com/alvarofonteles/py_functions_duno" },
                { name: "üîó py_collections_duno", url: "https://github.com/alvarofonteles/py_collections_duno" },
                { name: "üîó python_otavio", url: "https://github.com/alvarofonteles/python_otavio" },
                { name: "‚≠ê py_oop_otavio", url: "https://github.com/alvarofonteles/py_oop_otavio" },
                { name: "üîó py_oop_duno", url: "https://github.com/alvarofonteles/py_oop_duno" },
                { name: "üîó py_restapi_hashtag", url: "https://github.com/alvarofonteles/py_restapi_hashtag" },
            ],
            featured: true,
            status: "completed",
            images: [ "/projects/assets/images/badges/python-badge.svg" ]
        },
        {
            id: 2,
            title: "üìä FASE 2 - Pandas + ETL",
            description: "Dom√≠nio do Pandas para manipula√ß√£o de dados: desde Series/DataFrames b√°sicos at√© opera√ß√µes avan√ßadas como merge, groupby, pivot e conex√£o com bancos SQL para prepara√ß√£o de dados e ETL.",
            technologies: ["Python", "Pandas", "ETL", "Data Preparation", "Data Cleaning", "DataFrames", "Series", "GroupBy", "Merge", "Pivot", "SQL", "CSV", "Excel", "JSON"],
            links: [
                { name: "üîó pandas_samuka", url: "https://github.com/alvarofonteles/pandas_samuka" },
                // { name: "‚≠ê pandas_teo", url: "https://github.com/alvarofonteles/pandas_teo" },
            ],
            featured: true,
            status: "in-progress",
            images: [ "/projects/assets/images/badges/pandas-badge.svg" ]
        },
        {
            id: 3,
            title: "‚ö° FASE 3 - PySpark Fundamentos",
            description: "Processamento distribu√≠do com PySpark: cria√ß√£o de DataFrames, Spark SQL e transi√ß√£o suave de Pandas para Big Data.",
            technologies: ["PySpark", "DataFrames", "Spark SQL", "Distributed Computing", "ETL"],
            links: [],
            featured: true,
            status: "planned",
            images: [ "/projects/assets/images/badges/pyspark-badge.svg" ]
        },
        {
            id: 4,
            title: "üîç FASE 4 - Databricks Platform",
            description: "Plataforma enterprise completa: clusters, notebooks, Unity Catalog, workflows e governan√ßa de dados. Aqui voc√™ aprende a usar Databricks como ambiente central para Big Data.",
            technologies: ["Databricks", "Unity Catalog", "Spark SQL", "Workflows", "Data Governance", "Data Engineering"],            
            links: [],
            featured: true,
            status: "planned",
            images: [ "/projects/assets/images/badges/databricks-badge.svg" ]
        },
        {
            id: 5,
            title: "‚ö° FASE 5 - PySpark Avan√ßado + Delta Lake",
            description: "Pipelines production-ready com PySpark avan√ßado: otimiza√ß√£o, Delta Lake e qualidade de dados em escala. Agora sim faz sentido aplicar Delta Lake dentro do Databricks.",
            technologies: ["PySpark", "Delta Lake", "Data Quality", "Performance", "Optimization", "ACID Transactions", "Schema Evolution"],
            links: [],
            featured: true,
            status: "planned",
            images: [ 
                "/projects/assets/images/badges/pyspark-advanced-badge.svg",               
                "/projects/assets/images/badges/delta-lake-badge.svg",
            ]
        },
        {
            id: 6,
            title: "üèóÔ∏è FASE 6 - Lakehouse Architecture",
            description: "Arquitetura Lakehouse moderna: integra√ß√£o do Delta Lake com a medallion architecture, garantindo escalabilidade, confiabilidade e governan√ßa em pipelines corporativos.",
            technologies: ["Delta Lake", "Medallion Architecture", "ACID Transactions", "Schema Evolution", "Data Governance", "Data Pipelines"],   
            links: [],
            featured: true,
            status: "planned",
            images: [ 
                "/projects/assets/images/badges/lakehouse-badge.svg",                
                "/projects/assets/images/badges/delta-lake-badge.svg",
            ]
        },
        {
            id: 7,
            title: "üîÑ FASE 7 - Airflow Orchestration",
            description: "Orquestra√ß√£o de pipelines com Airflow: agendamento, monitoramento, dependencies e pipelines prontos para produ√ß√£o.",
            technologies: ["Airflow", "DAGs", "Orchestration", "Monitoring", "Scheduling"],
            links: [],
            featured: true,
            status: "planned",
            images: [ "/projects/assets/images/badges/airflow-badge.svg" ]
        },
        {
            id: 8,
            title: "‚òÅÔ∏è FASE 8 - AWS Data Stack",
            description: "Cloud computing para dados: S3 (storage), Glue (ETL serverless), Athena (query) e integra√ß√£o PySpark com AWS.",
            technologies: ["AWS S3", "AWS Glue", "AWS Athena", "boto3", "Cloud Computing"],
            links: [],
            featured: true,
            status: "planned",
            images: [ "/projects/assets/images/badges/aws-badge.svg" ]
        },
        {
            id: 9,
            title: "üìä FASE 9 - dbt Analytics Engineering",
            description: "Engenharia analytics moderna: camada de transforma√ß√£o com dbt, documenta√ß√£o, testes e data quality.",
            technologies: ["dbt", "Data Transformation", "Analytics Engineering", "Documentation", "Testing", "Data Quality"],
            links: [],
            featured: true,
            status: "planned",
            images: [ "/projects/assets/images/badges/dbt-badge.svg" ]
        }

    ],

    // Projetos Hands-on Data Engineering
    handsOn: [
        {
            id: 1,
            title: "üìä Analisador de Dados Python",
            subtitle: "FASE 1-2: üêç Python + Pandas",  // Novo campo
            description: "Conjunto de ferramentas para prepara√ß√£o e processamento de dados usando Python puro e Pandas.",
            technologies: ["Python", "OOP", "Pandas", "ETL", "Data Preparation", "Data Cleaning"],            
            links: [
                { name: "‚≠ê Reposit√≥rio", url: "https://github.com/alvarofonteles/analisador-dados-python" },
            ],
            featured: true,
            status: "in-progress",
            images: [ 
                "/projects/assets/images/badges/python-badge.svg",
                "/projects/assets/images/badges/pandas-badge.svg"
             ] // Dom√≠nio dos fundamentos
        },
        {
            id: 2,
            title: "üîÑ Pipeline ETL - Arquitetura Tradicional",
            subtitle: "FASE 3-4: ‚ö° PySpark + Databricks",
            description: "Pipeline ETL cl√°ssico: extra√ß√£o, transforma√ß√£o e carga, implementado com PySpark dentro do Databricks. Demonstra fundamentos s√≥lidos de engenharia de dados distribu√≠da.",
            technologies: ["PySpark", "Databricks", "Airflow", "Python", "SQL"],
            links: [
                // { name: "üîó pipeline-etl-tradicional", url: "https://github.com/alvarofonteles/pipeline-etl-tradicional" },
            ],
            featured: true,
            status: "planned",
            images: [ 
                "/projects/assets/images/badges/pyspark-badge.svg",
                "/projects/assets/images/badges/databricks-badge.svg" 
            ]
        },
        {
            id: 3,
            title: "‚ö° Pipeline ELT - Arquitetura Moderna",
            subtitle: "FASE 5+: üèóÔ∏è Delta Lake + Lakehouse",
            description: "Pipeline ELT moderno com Delta Lake: extra√ß√£o, carga direta no data lake e transforma√ß√£o sob demanda. Implementa arquitetura cloud-native com Databricks, dbt e AWS.",
            technologies: ["PySpark", "Delta Lake", "Databricks", "dbt", "AWS S3", "Airflow"],
            links: [
                // { name: "üîó pipeline-elt-moderno", url: "https://github.com/alvarofonteles/pipeline-elt-moderno" },
            ],
            featured: true,
            status: "planned",
            images: [ 
                "/projects/assets/images/badges/pyspark-advanced-badge.svg",
                "/projects/assets/images/badges/databricks-badge.svg",
                "/projects/assets/images/badges/aws-badge.svg"
            ]
        }
    ]    
}; 