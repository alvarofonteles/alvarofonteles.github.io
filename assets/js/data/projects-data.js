// ===== DADOS DOS PROJETOS - ROADMAP DATA ENGINEER =====
const PROJECTS_DATA = {
    // Projetos de Aprendizado
    learning: [
        {
            id: 0,
            title: "ğŸ—ï¸ Metodologia de Estudos",
            description: "Estrutura completa organizada em 9 fases para dominar Data Engineering moderno. Mostra minha abordagem sistemÃ¡tica de aprendizado.",
            technologies: ["OrganizaÃ§Ã£o", "Metodologia", "Roadmap", "Planejamento", "Data Engineering"],
            links: [
                {
                    name: "ğŸ“ Estrutura do Projeto",
                    url: "https://1drv.ms/f/c/0cc82fec9c1ab050/Ej_zAkJcDsxPpKpQkODTdqYBUnji_yfcbF2PlTOsoQmfOA"
                }
            ],
            featured: true,
            status: "completed",  // Porque a organizaÃ§Ã£o jÃ¡ estÃ¡ feita!
            image: "assets/images/projects/badges/methodology-badge.svg",
        },
        {
            id: 1,
            title: "ğŸ FASE 1 - Python Fundamentos",
            description: "Base sÃ³lida de programaÃ§Ã£o Python para Data Engineering: funÃ§Ãµes, OOP, collections, APIs e programaÃ§Ã£o concorrente.",
            technologies: ["Python", "OOP", "Data Structures", "Functions", "APIs", "Concurrency", "Dataclasses", "Threading", "Requests", "JSON"],
            links: [
                { name: "â­ python_teo", url: "https://github.com/alvarofonteles/python_teo" },
                { name: "ğŸ”— py_functions_duno", url: "https://github.com/alvarofonteles/py_functions_duno" },
                { name: "ğŸ”— py_collections_duno", url: "https://github.com/alvarofonteles/py_collections_duno" },
                { name: "ğŸ”— python_otavio", url: "https://github.com/alvarofonteles/python_otavio" },
                { name: "â­ py_oop_otavio", url: "https://github.com/alvarofonteles/py_oop_otavio" },
                { name: "ğŸ”— py_oop_duno", url: "https://github.com/alvarofonteles/py_oop_duno" },
                { name: "ğŸ”— py_api_duno", url: "https://github.com/alvarofonteles/py_api_duno" },
                // { name: "ğŸ”— py_threads_duno", url: "https://github.com/alvarofonteles/py_threads_duno" },
            ],
            featured: true,
            status: "in-progress",
            image: "assets/images/projects/badges/python-badge.svg",
        },
        {
            id: 2,
            title: "ğŸ“Š FASE 2 - Pandas + ETL",
            description: "ManipulaÃ§Ã£o de dados em memÃ³ria com Pandas: ETL de datasets mÃ©dios, anÃ¡lise exploratÃ³ria e preparaÃ§Ã£o para PySpark.",
            technologies: ["Python", "Pandas", "ETL", "Data Analysis", "Data Cleaning", "Data Visualization", "CSV", "Excel", "JSON", "DataFrames"],
            links: [
                // { name: "ğŸ”— pandas_samuka", url: "https://github.com/alvarofonteles/pandas_samuka" },
                // { name: "ğŸ”— pandas_teo", url: "https://github.com/alvarofonteles/pandas_teo" },
            ],
            featured: true,
            status: "planned",
            image: "assets/images/projects/badges/pandas-badge.svg",
        },
        {
            id: 3,
            title: "âš¡ FASE 3 - PySpark Fundamentos",
            description: "Processamento distribuÃ­do com PySpark: DataFrames, Spark SQL e transiÃ§Ã£o suave de Pandas para Big Data.",
            technologies: ["PySpark", "DataFrames", "Spark SQL", "Distributed Computing", "ETL"],
            links: [],
            featured: true,
            status: "planned",
            image: "assets/images/projects/badges/pyspark-badge.svg",
        },
        {
            id: 4,
            title: "âš¡ FASE 4 - PySpark AvanÃ§ado + Delta Lake",
            description: "Pipelines production-ready com PySpark avanÃ§ado: otimizaÃ§Ã£o, Delta Lake e qualidade de dados em escala.",
            technologies: ["PySpark", "Delta Lake", "Data Quality", "Performance", "Optimization"],
            links: [],
            featured: true,
            status: "planned",
            image: "assets/images/projects/badges/pyspark-advanced-badge.svg",
        },
        {
            id: 5,
            title: "ğŸ—ï¸ FASE 5 - Lakehouse Architecture",
            description: "Arquitetura Lakehouse moderna: Data Lake confiÃ¡vel com ACID transactions, schema evolution e medallion architecture.",
            technologies: ["Delta Lake", "Medallion Architecture", "ACID Transactions", "Schema Evolution", "Data Governance"],
            links: [],
            featured: true,
            status: "planned",
            image: "assets/images/projects/badges/lakehouse-badge.svg",
        },
        {
            id: 6,
            title: "ğŸ”„ FASE 6 - Airflow Orchestration",
            description: "Orchestration de pipelines com Airflow: agendamento, monitoramento, dependencies e pipelines production-ready.",
            technologies: ["Airflow", "DAGs", "Orchestration", "Monitoring", "Scheduling"],
            links: [],
            featured: true,
            status: "planned",
            image: "assets/images/projects/badges/airflow-badge.svg",
        },
        {
            id: 7,
            title: "â˜ï¸ FASE 7 - AWS Data Stack",
            description: "Cloud computing para dados: S3 (storage), Glue (ETL serverless), Athena (query) e integraÃ§Ã£o PySpark com AWS.",
            technologies: ["AWS S3", "AWS Glue", "AWS Athena", "boto3", "Cloud Computing"],
            links: [],
            featured: true,
            status: "planned",
            image: "assets/images/projects/badges/aws-badge.svg",
        },
        {
            id: 8,
            title: "ğŸ“Š FASE 8 - dbt Analytics Engineering",
            description: "Engenharia analytics moderna: camada transformaÃ§Ã£o com dbt, documentaÃ§Ã£o, testes e data quality.",
            technologies: ["dbt", "Data Transformation", "Analytics Engineering", "Documentation", "Testing", "Data Quality"],
            links: [],
            featured: true,
            status: "planned",
            image: "assets/images/projects/badges/dbt-badge.svg",
        },
        {
            id: 9,
            title: "ğŸ” FASE 9 - Databricks Platform",
            description: "Plataforma enterprise completa: Lakehouse unified, CI/CD, governance e Unity Catalog.",
            technologies: ["Databricks", "Unity Catalog", "Spark SQL", "Workflows", "Data Governance"],
            links: [],
            featured: true,
            status: "planned",
            image: "assets/images/projects/badges/databricks-badge.svg",
        },
    ],

    // Projetos Data Engineering
    portfolio: [
        {
            id: "portfolio-template",
            title: "ğŸš€ Portfolio Template - GitHub Pages",
            subtitle: "Template Moderno para Desenvolvedores",
            description: "Modelo de portfolio profissional moderno e responsivo, desenvolvido com HTML5, CSS3 e JavaScript vanilla.",
            technologies: ["HTML5", "CSS3", "JavaScript", "Vanilla JS", "SEO", "GitHub Pages", "Responsive Design", "CSS Grid", "Flexbox"],
            links: [
                { name: "â­ RepositÃ³rio", url: "https://github.com/alvarofonteles/alvarofonteles.github.io" },
                { name: "ğŸŒ Live Demo", url: "https://alvarofonteles.github.io" },
                { name: "ğŸ“– README", url: "https://github.com/alvarofonteles/alvarofonteles.github.io#readme" }
            ],
            featured: true,
            status: "completed",
            image: "assets/images/projects/badges/portfolio-badge.svg",
            architecture: "Static Site",
            highlights: [
                "100% Vanilla JavaScript - Zero dependÃªncias",
                "Deploy automÃ¡tico com GitHub Pages", 
                "Design system com CSS Variables",
                "Estrutura modular e escalÃ¡vel",
                "Performance otimizada e SEO ready"
            ]
        },
        {
            id: 1,
            title: "ğŸ“Š Analisador de Dados Python",
            subtitle: "FASE 1-2: ğŸ Python + Pandas",  // Novo campo
            description: "Conjunto de ferramentas para anÃ¡lise e processamento de dados usando Python puro e Pandas.",
            technologies: ["Python", "OOP", "Pandas", "ETL", "Data Analysis", "Data Visualization"],
            links: [
                { name: "â­ analisador-dados-python", url: "https://github.com/alvarofonteles/analisador-dados-python" },
            ],
            featured: true,
            status: "in-progress",
            image: "assets/images/projects/badges/python-badge.svg",
            architecture: "ETL" // DomÃ­nio dos fundamentos
        },
        {
            id: 2,
            title: "ğŸ”„ Pipeline ETL - Arquitetura Tradicional",
            subtitle: "FASE 3-4: âš¡ PySpark + Delta",
            description: "Pipeline ETL clÃ¡ssico: extraÃ§Ã£o, transformaÃ§Ã£o e depois carga. Demonstra fundamentos sÃ³lidos de engenharia de dados.",
            technologies: ["PySpark", "Delta Lake", "Airflow", "Python", "SQL"],
            links: [
                // { name: "ğŸ”— pipeline-etl-tradicional", url: "https://github.com/alvarofonteles/pipeline-etl-tradicional" },
            ],
            featured: true,
            status: "planned",
            image: "assets/images/projects/badges/pyspark-badge.svg",
            architecture: "ETL" // Habilidades Spark tradicionais
        },
        {
            id: 3,
            title: "âš¡ Pipeline ELT - Arquitetura Moderna",
            subtitle: "FASE 5+: ğŸ—ï¸ Arquitetura Moderna",
            description: "Pipeline ELT moderno com Delta Lake: extraÃ§Ã£o, carga direta no data lake e transformaÃ§Ã£o sob demanda. Arquitetura cloud-native.",
            technologies: ["PySpark", "Delta Lake", "dbt", "AWS S3", "Databricks", "Airflow"],
            links: [
                // { name: "ğŸ”— pipeline-elt-moderno", url: "https://github.com/alvarofonteles/pipeline-elt-moderno" },
            ],
            featured: true,
            status: "planned",
            image: "assets/images/projects/badges/delta-lake-badge.svg",
            architecture: "ELT" // Arquitetura cloud-native
        }
    ],
};