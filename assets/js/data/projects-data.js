// ===== DADOS DOS PROJETOS - ROADMAP DATA ENGINEER =====
const PROJECTS_DATA = [
    {
        id: 0,
        title: "üèóÔ∏è Metodologia de Estudos",
        description: "Estrutura completa organizada em 9 fases para dominar Data Engineering moderno. Mostra minha abordagem sistem√°tica de aprendizado.",
        technologies: ["Organiza√ß√£o", "Metodologia", "Roadmap", "Planejamento", "Data Engineering"],
        links: [
            { 
                name: "üëÅÔ∏è Ver Estrutura Completa", 
                url: "https://1drv.ms/f/c/0cc82fec9c1ab050/Ej_zAkJcDsxPpKpQkODTdqYBUnji_yfcbF2PlTOsoQmfOA" 
            }
        ],
        featured: true,
        status: "completed",  // Porque a organiza√ß√£o j√° est√° feita!
        image: "assets/images/projects/methodology-structure.jpg"
    },
    {
        id: 1,
        title: "üêç FASE 1 - Python Fundamentos",
        description: "Base s√≥lida de programa√ß√£o Python para Data Engineering: fun√ß√µes, OOP, collections, APIs e programa√ß√£o concorrente.",
        technologies: ["Python", "OOP", "Data Structures", "Functions", "APIs", "Concurrency", "Dataclasses"],
        links: [
            { name: "python-teo", url: "https://github.com/alvarofonteles/python-teo" },
            { name: "py-functions-duno", url: "https://github.com/alvarofonteles/py-functions-duno" },
            { name: "py-collections-duno", url: "https://github.com/alvarofonteles/py-collections-duno" }
        ],
        featured: true,
        status: "in-progress",
        image: "assets/images/projects/python-fundamentals.jpg"
    },
    {
        id: 2,
        title: "üìä FASE 2 - Pandas + ETL",
        description: "Manipula√ß√£o de dados em mem√≥ria com Pandas: ETL de datasets m√©dios, an√°lise explorat√≥ria e prepara√ß√£o para PySpark.",
        technologies: ["Python", "Pandas", "ETL", "Data Analysis", "Data Cleaning", "Data Visualization"],
        links: [
            { name: "pandas-teo", url: "https://github.com/alvarofonteles/pandas-teo" }
        ],
        featured: true,
        status: "planned",
        image: "assets/images/projects/pandas-analysis.jpg"
    },
    {
        id: 3,
        title: "‚ö° FASE 3 - PySpark Fundamentos",
        description: "Processamento distribu√≠do com PySpark: DataFrames, Spark SQL e transi√ß√£o suave de Pandas para Big Data.",
        technologies: ["PySpark", "DataFrames", "Spark SQL", "Distributed Computing", "ETL"],
        links: [],
        featured: true,
        status: "planned",
        image: "assets/images/projects/pyspark-intro.jpg"
    },
    {
        id: 4,
        title: "‚ö° FASE 4 - PySpark Avan√ßado + Delta Lake",
        description: "Pipelines production-ready com PySpark avan√ßado: otimiza√ß√£o, Delta Lake e qualidade de dados em escala.",
        technologies: ["PySpark", "Delta Lake", "Data Quality", "Performance", "Optimization"],
        links: [],
        featured: true,
        status: "planned",
        image: "assets/images/projects/pyspark-pipeline.jpg"
    },
    {
        id: 5,
        title: "üèóÔ∏è FASE 5 - Lakehouse Architecture",
        description: "Arquitetura Lakehouse moderna: Data Lake confi√°vel com ACID transactions, schema evolution e medallion architecture.",
        technologies: ["Delta Lake", "Medallion Architecture", "ACID Transactions", "Schema Evolution", "Data Governance"],
        links: [],
        featured: true,
        status: "planned",
        image: "assets/images/projects/data-lakehouse.jpg"
    },
    {
        id: 6,
        title: "üîÑ FASE 6 - Airflow Orchestration",
        description: "Orchestration de pipelines com Airflow: agendamento, monitoramento, dependencies e pipelines production-ready.",
        technologies: ["Airflow", "DAGs", "Orchestration", "Monitoring", "Scheduling"],
        links: [],
        featured: true,
        status: "planned",
        image: "assets/images/projects/airflow-dags.jpg"
    },
    {
        id: 7,
        title: "‚òÅÔ∏è FASE 7 - AWS Data Stack",
        description: "Cloud computing para dados: S3 (storage), Glue (ETL serverless), Athena (query) e integra√ß√£o PySpark com AWS.",
        technologies: ["AWS S3", "AWS Glue", "AWS Athena", "boto3", "Cloud Computing"],
        links: [],
        featured: true,
        status: "planned",
        image: "assets/images/projects/aws-data-stack.jpg"
    },
    {
        id: 8,
        title: "üìä FASE 8 - dbt Analytics Engineering",
        description: "Engenharia analytics moderna: camada transforma√ß√£o com dbt, documenta√ß√£o, testes e data quality.",
        technologies: ["dbt", "Data Transformation", "Analytics Engineering", "Documentation", "Testing", "Data Quality"],
        links: [],
        featured: true,
        status: "planned",
        image: "assets/images/projects/dbt-transformations.jpg"
    },
    {
        id: 9,
        title: "üîç FASE 9 - Databricks Platform",
        description: "Plataforma enterprise completa: Lakehouse unified, CI/CD, governance e Unity Catalog.",
        technologies: ["Databricks", "Unity Catalog", "Spark SQL", "Workflows", "Data Governance"],
        links: [],
        featured: true,
        status: "planned",
        image: "assets/images/projects/databricks-spark.jpg"
    }
];